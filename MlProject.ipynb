{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn import metrics\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from scipy import stats\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas.plotting import parallel_coordinates\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import Lasso\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier\n",
        "from scipy.stats import uniform, reciprocal\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from pickle import dump,load\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from xgboost import XGBRegressor"
      ],
      "metadata": {
        "id": "jRXUJ_iSpEQa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/ApartmentRentPrediction_Milestone2.csv')"
      ],
      "metadata": {
        "id": "V3E_wgqVCwjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**check nulls and redudent records**"
      ],
      "metadata": {
        "id": "W_oRaVE9C06Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check dublicates\n",
        "duplicate_rows = df.duplicated()\n",
        "redundant_records_count = duplicate_rows.sum()\n",
        "print(f\"redundant records: {redundant_records_count}\")\n",
        "\n",
        "#check nulls\n",
        "for column in df.columns:\n",
        "    null_count = df[column].isnull().sum()\n",
        "    print(f\"Null count for column '{column}': {null_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8r6Oyt_GpHQT",
        "outputId": "2aaf3ead-ccb9-4ec3-872b-a1fe8f7e0d06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "redundant records: 0\n",
            "Null count for column 'id': 0\n",
            "Null count for column 'category': 0\n",
            "Null count for column 'title': 0\n",
            "Null count for column 'body': 0\n",
            "Null count for column 'amenities': 3185\n",
            "Null count for column 'bathrooms': 30\n",
            "Null count for column 'bedrooms': 7\n",
            "Null count for column 'currency': 0\n",
            "Null count for column 'fee': 0\n",
            "Null count for column 'has_photo': 0\n",
            "Null count for column 'pets_allowed': 3751\n",
            "Null count for column 'RentCategory': 0\n",
            "Null count for column 'price_type': 0\n",
            "Null count for column 'square_feet': 0\n",
            "Null count for column 'address': 2971\n",
            "Null count for column 'cityname': 66\n",
            "Null count for column 'state': 66\n",
            "Null count for column 'latitude': 7\n",
            "Null count for column 'longitude': 7\n",
            "Null count for column 'source': 0\n",
            "Null count for column 'time': 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "convert title and body columns to its length"
      ],
      "metadata": {
        "id": "QXVSkpZwC_Js"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def body_and_title(df):\n",
        "  i = 0\n",
        "  for x in df['title']:\n",
        "    if isinstance(x,str):\n",
        "       df.loc[i,'title'] = len(x)\n",
        "    else:\n",
        "       df.loc[i,'title'] = 1\n",
        "    i = i + 1\n",
        "\n",
        "  i = 0\n",
        "  for x in df['body']:\n",
        "    if isinstance(x,str):\n",
        "       df.loc[i,'body'] = len(x)\n",
        "    else:\n",
        "       df.loc[i,'body'] = 1\n",
        "    i = i + 1\n",
        "  return df\n",
        "\n",
        "df = body_and_title(df)\n",
        "\n",
        "with open('body_and_title.pkl', 'wb') as f:\n",
        "    pickle.dump(body_and_title, f)\n"
      ],
      "metadata": {
        "id": "3UahmKEupMzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing cityname column"
      ],
      "metadata": {
        "id": "As-WC-sjDEj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_mode_cityname():\n",
        "  return 'Austin'\n",
        "\n",
        "def frequency_map(df):\n",
        "  # frequancy encoding\n",
        "  frequency_map = df['cityname'].value_counts(normalize=True)\n",
        "  df['cityname'] = df['cityname'].map(frequency_map)\n",
        "  return df\n",
        "\n",
        "df = frequency_map(df)\n",
        "\n",
        "with open('/content/frequency_map.pkl', 'wb') as f:\n",
        "     pickle.dump(frequency_map, f)\n",
        "\n",
        "\n",
        "with open('get_mode_cityname.pkl', 'wb') as f:\n",
        "    pickle.dump(get_mode_cityname, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "FaEFgnYxpQ8m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing amenities column"
      ],
      "metadata": {
        "id": "sdPieHqvDNs7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_amenities_column(df):\n",
        "    cnt = -1\n",
        "    num = 0\n",
        "    total_sum = 0\n",
        "\n",
        "    my_set = set()\n",
        "    my_dict = {}\n",
        "    for x in df['amenities']:\n",
        "        cnt = cnt + 1\n",
        "        if isinstance(x, str):\n",
        "            s = \"\"\n",
        "            for c in x:\n",
        "                s = s + c\n",
        "                if c == ',':\n",
        "                    s = s[:-1]\n",
        "                    if s in my_dict:\n",
        "                        my_dict[s] = my_dict[s] + 1\n",
        "                    else:\n",
        "                        my_dict[s] = 1\n",
        "                    my_set.add(s)\n",
        "                    s = \"\"\n",
        "            my_set.add(s)\n",
        "            if s in my_dict:\n",
        "                my_dict[s] = my_dict[s] + 1\n",
        "            else:\n",
        "                my_dict[s] = 1\n",
        "\n",
        "    dict_length = len(my_dict)\n",
        "    my_dict = {k: v for k, v in sorted(my_dict.items(), key=lambda item: item[1])}\n",
        "\n",
        "    for key in my_dict:\n",
        "        my_dict[key] = 1 / my_dict[key]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    cnt = -1\n",
        "    for x in df['amenities']:\n",
        "        cnt = cnt + 1\n",
        "        count = 0\n",
        "        if isinstance(x, str):\n",
        "            s = \"\"\n",
        "            for c in x:\n",
        "                s = s + c\n",
        "                if c == ',':\n",
        "                    s = s[:-1]\n",
        "                    count = my_dict[s] + count\n",
        "                    s = \"\"\n",
        "            count = my_dict[s] + count\n",
        "            total_sum = total_sum + count\n",
        "            num = num + 1\n",
        "            df.at[cnt, 'amenities'] = count\n",
        "\n",
        "    mean = total_sum / num\n",
        "    df['amenities'].fillna(mean, inplace=True)\n",
        "    return df\n",
        "\n",
        "df = preprocess_amenities_column(df)\n",
        "\n",
        "\n",
        "with open('amenitie_function.pkl', 'wb') as f:\n",
        "    pickle.dump(preprocess_amenities_column, f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "R35HdGPkpaU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform label encoding based on the condition\n",
        "def RentCategory_encoding(df):\n",
        "  # Define the condition\n",
        "  condition = {'Low Rent': 1, 'Medium-Priced Rent': 2, 'High Rent': 3}\n",
        "  df['RentCategory'] = df['RentCategory'].map(condition).fillna(0).astype(int)\n",
        "  return df\n",
        "\n",
        "# Save the rentcategory encoding\n",
        "df=RentCategory_encoding(df)\n",
        "\n",
        "with open('RentCategory_encoding.pkl', 'wb') as f:\n",
        "    pickle.dump(RentCategory_encoding, f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "abAeyNoupeO9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Address column\n"
      ],
      "metadata": {
        "id": "py368FqIDXw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def addressfunc(df):\n",
        "  cnt = -1\n",
        "  for x in df['address']:\n",
        "    cnt = cnt + 1\n",
        "    if isinstance(x, str):\n",
        "      df.at[cnt, 'address'] = 1\n",
        "    else:\n",
        "      df.at[cnt, 'address'] = 0\n",
        "  return df\n",
        "df=addressfunc(df)\n",
        "\n",
        "with open('addressfunc.pkl', 'wb') as f:\n",
        "    pickle.dump(addressfunc, f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "ekksOaOIpiLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One Hot Encoding for 'category' , 'price_type' , 'has_photo' , 'fee' ,'currency' , 'RentCategory' columns.\n"
      ],
      "metadata": {
        "id": "s9loa1JZDd5C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mode_state_time = 'TX'\n",
        "\n",
        "def get_mode_state_time():\n",
        "  return mode_state_time\n",
        "\n",
        "  #Drop nulls in 'state' column\n",
        "df.dropna(subset=['state'], inplace=True)\n",
        "\n",
        "def columns_to_encode(df):\n",
        "  #Fill the nulls with 'no_pets' in pets_allowed column\n",
        "  df['pets_allowed'].fillna(\"NO_PETS\", inplace=True)\n",
        "\n",
        "  print(df['category'].unique())\n",
        "  print(df['price_type'].unique())\n",
        "  print(df['has_photo'].unique())\n",
        "  print(df['fee'].unique())\n",
        "  print(df['currency'].unique())\n",
        "  print(df['pets_allowed'].unique())\n",
        "  print(df['source'].unique())\n",
        "  print(df['state'].unique())\n",
        "\n",
        "  columns_to_encode = ['category','price_type','has_photo','fee' , 'currency', 'source' , 'pets_allowed','state']\n",
        "  # Initialize the OneHotEncoder\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "  # Fit and transform the data\n",
        "  encoded_data = encoder.fit_transform(df[columns_to_encode])\n",
        "  # Create a DataFrame from the encoded data\n",
        "  encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns_to_encode))\n",
        "  # Concatenate the original DataFrame with the encoded DataFrame\n",
        "  result_df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "\n",
        "  #drop the original columns except 'state'\n",
        "  df = result_df.drop(columns=['category','price_type','has_photo','fee' , 'currency', 'source' , 'pets_allowed'])\n",
        "  return df\n",
        "\n",
        "df=columns_to_encode(df)\n",
        "with open('columns_to_encode.pkl', 'wb') as f:\n",
        "    pickle.dump(columns_to_encode, f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I6KerlBTpt45",
        "outputId": "5dc46725-07c5-4ffd-8e3a-da1fdd243a55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['housing/rent/apartment' 'housing/rent/short_term' 'housing/rent/home']\n",
            "['Monthly' 'Weekly' 'Monthly|Weekly']\n",
            "['Thumbnail' 'Yes' 'No']\n",
            "['No']\n",
            "['USD']\n",
            "['Cats,Dogs' 'NO_PETS' 'Dogs' 'Cats']\n",
            "['RentDigs.com' 'RentLingo' 'RealRentals' 'ListedBuy' 'Listanza'\n",
            " 'RENTCafé' 'RENTOCULAR' 'GoSection8' 'tenantcloud' 'Real Estate Agent'\n",
            " 'rentbits']\n",
            "['NC' 'WI' 'FL' 'NE' 'CA' 'LA' 'WA' 'OK' 'TX' 'MN' 'VA' 'IN' 'OR' 'OH'\n",
            " 'NJ' 'PA' 'ND' 'KS' 'IL' 'AK' 'MA' 'AZ' 'SC' 'MD' 'IA' 'CO' 'GA' 'NY'\n",
            " 'MO' 'TN' 'DC' 'MI' 'NH' 'UT' 'AR' 'CT' 'NV' 'RI' 'AL' 'SD' 'KY' 'VT'\n",
            " 'NM' 'MS' 'MT' 'ID' 'HI' 'WV' 'DE' 'WY']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna()\n",
        "filter_latitude_state = df[df.state == 'TX']\n",
        "mean_latitude = filter_latitude_state.latitude.mean()\n",
        "mean_longitude = filter_latitude_state.longitude.mean()\n",
        "def get_mean_x_y():\n",
        "  return mean_latitude , mean_longitude\n",
        "\n",
        "\n",
        "with open('get_mean_x_y.pkl', 'wb') as f:\n",
        "      pickle.dump(get_mean_x_y, f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "tuXWtet0pyPb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**bathrooms and bedrooms column**"
      ],
      "metadata": {
        "id": "pUPBlaZ2D-HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_rooms = 0\n",
        "def bathroom_bedroom_column(df):\n",
        "  #Replace float numbers with Integers.\n",
        "  df['bathrooms'] = df['bathrooms'].astype(int)\n",
        "  df['bedrooms'] = df['bedrooms'].astype(int)\n",
        "\n",
        "  #Generate new column \"rooms\" which is the summetion of bathrooms and bedrooms.\n",
        "\n",
        "  rooms = df['bathrooms'] + df['bedrooms']\n",
        "  df.insert(1, 'rooms', rooms)\n",
        "  mean_rooms = df['rooms'].mean()\n",
        "  mean_rooms = math.floor(mean_rooms)\n",
        "\n",
        "\n",
        "  # Generate new column \"Studio\"  , 1 if studio else 0.\n",
        "  studio = []\n",
        "  cnt = 0\n",
        "  for x in df['bedrooms']:\n",
        "    if x == 0:\n",
        "       studio.append(1)\n",
        "    else:\n",
        "       studio.append(0)\n",
        "    cnt = cnt + 1\n",
        "  df.insert(1, 'studio', studio)\n",
        "\n",
        "  #Generate new column \"room_ratio\" = square_feet / rooms.\n",
        "  new_col = df['square_feet'] / df['rooms']\n",
        "  df.insert(1, 'room_ratio', new_col)\n",
        "\n",
        "  #Drop bathrooms and bedrooms\n",
        "  df = df.drop(['bathrooms'], axis=1)\n",
        "  df = df.drop(['bedrooms'], axis=1)\n",
        "  return df\n",
        "df=bathroom_bedroom_column(df)\n",
        "\n",
        "# get mean rooms\n",
        "def get_mean_rooms():\n",
        "  return math.floor(mean_rooms)\n",
        "\n",
        "with open('get_mean_rooms.pkl', 'wb') as f:\n",
        "      pickle.dump(get_mean_rooms, f)\n",
        "\n",
        "with open('Bathroom_bedroom_column.pkl', 'wb') as f:\n",
        "      pickle.dump(bathroom_bedroom_column, f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "mimzJATzp3EM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7cc8d11e-61b5-42cf-e6b0-40d21b8a4ab2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-ea41ca943599>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['bathrooms'] = df['bathrooms'].astype(int)\n",
            "<ipython-input-11-ea41ca943599>:5: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  df['bedrooms'] = df['bedrooms'].astype(int)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change every state with its time zone and do OHE**"
      ],
      "metadata": {
        "id": "ckFwcrf7EAkg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NO6nHr8fJjEb",
        "outputId": "0c3a0460-688e-422a-bad3-f635da581f9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "with open('get_mode_state_time.pkl', 'wb') as f:\n",
        "    pickle.dump(get_mode_state_time, f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "def state_time_zones_func(df):\n",
        "  state_time_zones = {\n",
        "      'AL': 'UTC-6',\n",
        "      'AK': 'UTC-9',\n",
        "      'AZ': 'UTC-7',\n",
        "      'AR': 'UTC-6',\n",
        "      'CA': 'UTC-8',\n",
        "      'CO': 'UTC-7',\n",
        "      'CT': 'UTC-5',\n",
        "      'DE': 'UTC-5',\n",
        "      'FL': 'UTC-5',\n",
        "      'GA': 'UTC-5',\n",
        "      'HI': 'UTC-10',\n",
        "      'ID': 'UTC-7',\n",
        "      'IL': 'UTC-6',\n",
        "      'IN': 'UTC-5',\n",
        "      'IA': 'UTC-6',\n",
        "      'KS': 'UTC-6',\n",
        "      'KY': 'UTC-5',\n",
        "      'LA': 'UTC-6',\n",
        "      'ME': 'UTC-5',\n",
        "      'MD': 'UTC-5',\n",
        "      'MA': 'UTC-5',\n",
        "      'MI': 'UTC-5',\n",
        "      'MN': 'UTC-6',\n",
        "      'MS': 'UTC-6',\n",
        "      'MO': 'UTC-6',\n",
        "      'MT': 'UTC-7',\n",
        "      'NE': 'UTC-6',\n",
        "      'NV': 'UTC-8',\n",
        "      'NH': 'UTC-5',\n",
        "      'NJ': 'UTC-5',\n",
        "      'NM': 'UTC-7',\n",
        "      'NY': 'UTC-5',\n",
        "      'NC': 'UTC-5',\n",
        "      'ND': 'UTC-6',\n",
        "      'OH': 'UTC-5',\n",
        "      'OK': 'UTC-6',\n",
        "      'OR': 'UTC-8',\n",
        "      'PA': 'UTC-5',\n",
        "      'RI': 'UTC-5',\n",
        "      'SC': 'UTC-5',\n",
        "      'SD': 'UTC-6',\n",
        "      'TN': 'UTC-5' ,\n",
        "      'TX': 'UTC-6',\n",
        "      'UT': 'UTC-7',\n",
        "      'VT': 'UTC-5',\n",
        "      'VA': 'UTC-5',\n",
        "      'WA': 'UTC-8',\n",
        "      'WV': 'UTC-5',\n",
        "      'WI': 'UTC-6',\n",
        "      'WY': 'UTC-7',\n",
        "      'DC': 'UTC-5'\n",
        "  }\n",
        "\n",
        "\n",
        "  df = df.reset_index(drop=True)\n",
        "  cnt = 0\n",
        "  for i in df['state']:\n",
        "    df.loc[cnt , 'state'] = state_time_zones[i]\n",
        "    cnt = cnt + 1\n",
        "\n",
        "  columns_to_encode = ['state']\n",
        "  encoder = OneHotEncoder(sparse=False)\n",
        "  encoded_data = encoder.fit_transform(df[columns_to_encode])\n",
        "  encoded_df = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(columns_to_encode))\n",
        "  result_df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "  df = result_df.drop(columns=columns_to_encode)\n",
        "  return df\n",
        "\n",
        "df=state_time_zones_func(df)\n",
        "with open('state_time_zones.pkl', 'wb') as f:\n",
        "    pickle.dump(state_time_zones_func, f)\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QodwGOH0TzjM"
      },
      "outputs": [],
      "source": [
        "# Initialize the MinMaxScaler\n",
        "def minMaxScaler(df):\n",
        "  scaler = MinMaxScaler()\n",
        "  columns_to_normalize = ['id','time' , 'title' , 'body','cityname']\n",
        "  df[columns_to_normalize] = scaler.fit_transform(df[columns_to_normalize])\n",
        "  return df\n",
        "\n",
        "df=minMaxScaler(df)\n",
        "with open('minmaxscaler.pkl', 'wb') as f:\n",
        "    pickle.dump(minMaxScaler, f)\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yabdcx9rT-6y"
      },
      "outputs": [],
      "source": [
        "# Outliers\n",
        "def remove_outliers_iqr(df, column):\n",
        "  q1 = df[column].quantile(0.25)\n",
        "  q3 = df[column].quantile(0.75)\n",
        "  iqr = q3 - q1\n",
        "  lower_bound = q1 - 1.5 * iqr\n",
        "  upper_bound = q3 + 1.5 * iqr\n",
        "  return df[(df[column] > lower_bound) & (df[column] < upper_bound)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "98b9yEwNuQLN"
      },
      "outputs": [],
      "source": [
        "# Decoding\n",
        "# Define the condition\n",
        "def decodefunc(df):\n",
        "  condition = { 1:'Low Rent', 2:'Medium-Priced Rent', 3 : 'High Rent'}\n",
        "  # Perform label encoding based on the condition\n",
        "  df['RentCategory'] = df['RentCategory'].map(condition).fillna(0).astype(str)\n",
        "  return df\n",
        "\n",
        "# Save the decoding of RentCategory\n",
        "with open('RentCategory_Decode.pkl', 'wb') as f:\n",
        "    pickle.dump(decodefunc, f)\n",
        "f.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5M-S-ccGUFGE",
        "outputId": "8d47d517-5916-4a08-a15a-9442f8a42d68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['rooms', 'square_feet', 'address', 'cityname', 'latitude', 'longitude',\n",
            "       'state_UTC-8', 'state_UTC-9'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "df = df.drop(['state_UTC-6'], axis=1)\n",
        "df = df.drop(['state_UTC-10'], axis=1)\n",
        "\n",
        "def correlationfunc(df):\n",
        "  df = df.drop(['id'], axis=1)\n",
        "  df = df.drop(['time'], axis=1)\n",
        "  df = df.drop(['body'], axis=1)\n",
        "  df = df.drop(['title'], axis=1)\n",
        "  df = df.drop(['room_ratio'], axis=1)\n",
        "  df = df.reset_index(drop=True)\n",
        "  corr = df.corr()\n",
        "  top_feature = corr.index[abs(corr['RentCategory']) >= 0.03]\n",
        "  top_corr = df[top_feature].corr()\n",
        "  top_feature = top_feature.drop(['RentCategory'])\n",
        "  return top_feature\n",
        "\n",
        "top_feature=correlationfunc(df)\n",
        "df = decodefunc(df)\n",
        "\n",
        "# Save the Correlation\n",
        "with open('correlation.pkl', 'wb') as f:\n",
        "    pickle.dump(correlationfunc, f)\n",
        "f.close()\n",
        "\n",
        "print(top_feature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NHoOcE82Ybn7"
      },
      "outputs": [],
      "source": [
        "def timer(start_time=None):\n",
        "    if not start_time:\n",
        "        start_time = datetime.now()\n",
        "        return start_time\n",
        "    elif start_time:\n",
        "        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n",
        "        tmin, tsec = divmod(temp_sec, 60)\n",
        "        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVM\n"
      ],
      "metadata": {
        "id": "1jhFq8TMeWkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4B1aRLyf61s",
        "outputId": "eb9cfc49-6e17-497e-c08f-29584325894e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
            "Best Parameters: {'gamma': 0.001, 'C': 100}\n",
            "Best Score: 0.6778938949399944\n",
            "Accuracy on Test Set: 0.6796830786644029\n"
          ]
        }
      ],
      "source": [
        "start_time = timer(None) # timing starts from this point for \"start_time\" variable\n",
        "\n",
        "X = df[top_feature]\n",
        "y = df['RentCategory']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "svm_clf = SVC()\n",
        "# Define the hyperparameters to tune\n",
        "param_dist = {\n",
        "    'C': [0.1, 1, 10 , 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    }\n",
        "\n",
        "# Randomized search with cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=svm_clf, param_distributions=param_dist, n_iter=10, cv=5, n_jobs=-1, verbose=2)\n",
        "\n",
        "# Perform hyperparameter tuning\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "print(\"Best Parameters:\", best_params)\n",
        "print(\"Best Score:\", best_score)\n",
        "\n",
        "# Evaluate the model on the test set using the best parameters\n",
        "best_svm_clf = SVC(**best_params)\n",
        "best_svm_clf.fit(X_train, y_train)\n",
        "y_pred = best_svm_clf.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy on Test Set:\",accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "decision tree classifier\n"
      ],
      "metadata": {
        "id": "hLfWJARCo3SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "param_grid = {\n",
        "    'max_depth': [None, 10, 20, 30]\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier\n",
        "dt_classifier = DecisionTreeClassifier(random_state=20)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(dt_classifier, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding scores\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_dt_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on the test set\n",
        "dt_pred = best_dt_classifier.predict(X_test)\n",
        "\n",
        "#Save the best lr classifier using pickle\n",
        "with open('best_dt_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(best_dt_classifier, f)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Decision Tree Classifier (Tuned):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, dt_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3Ew6xrGqadN",
        "outputId": "dda5c50f-d7d8-44a2-e2f5-503ecbde8f58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 10}\n",
            "Best Score: 0.7417199954754348\n",
            "Decision Tree Classifier (Tuned):\n",
            "Accuracy: 0.7402376910016978\n",
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "         High Rent       0.77      0.72      0.74       275\n",
            "          Low Rent       0.69      0.71      0.70       533\n",
            "Medium-Priced Rent       0.76      0.77      0.76       959\n",
            "\n",
            "          accuracy                           0.74      1767\n",
            "         macro avg       0.74      0.73      0.74      1767\n",
            "      weighted avg       0.74      0.74      0.74      1767\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random Forest\n"
      ],
      "metadata": {
        "id": "OtbDym1Ao9K1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYoVgSORfyu8",
        "outputId": "2c56b5d7-411b-4d92-9ceb-78e899ae9c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'max_depth': 20, 'n_estimators': 200}\n",
            "Best Score: 0.7725716247693923\n",
            "Accuracy: 0.7685342388228636\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 200],\n",
        "    'max_depth': [None, 10, 20]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(rf_classifier, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding scores\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_rf_classifier = grid_search.best_estimator_\n",
        "#Save the best rf classifier using pickle\n",
        "with open('best_rf_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(best_rf_classifier, f)\n",
        "# Predictions on the test set\n",
        "rf_pred = best_rf_classifier.predict(X_test)\n",
        "\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "logistic"
      ],
      "metadata": {
        "id": "9g8CKyE_o_bx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGWds5UDfbuu",
        "outputId": "6c5a887d-0f5a-45b3-ae5b-cc5e85e4dc75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'C': 10, 'penalty': 'l2'}\n",
            "Best Score: 0.6450600656061967\n",
            "Logistic Regression Classifier (Tuned):\n",
            "Accuracy: 0.6598754951895869\n"
          ]
        }
      ],
      "source": [
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "lr_classifier = LogisticRegression(random_state=42, solver='liblinear', max_iter=10000)\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(lr_classifier, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding scores\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_lr_classifier = grid_search.best_estimator_\n",
        "#Save the best lr classifier using pickle\n",
        "with open('best_lr_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(best_lr_classifier, f)\n",
        "# Predictions on the test set\n",
        "lr_pred = best_lr_classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the classifier\n",
        "print(\"Logistic Regression Classifier (Tuned):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "SkPC8mPjpD2C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUZdvUJnuFU3",
        "outputId": "810068b2-a172-4f3b-82f3-7e3101bd4fc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Hyperparameters: {'metric': 'manhattan', 'n_neighbors': 9, 'weights': 'distance'}\n",
            "Best Score: 0.6729402967594302\n",
            "KNN Classifier (Tuned):\n",
            "Accuracy: 0.7119411431805319\n",
            "Classification Report:\n",
            "                     precision    recall  f1-score   support\n",
            "\n",
            "         High Rent       0.67      0.61      0.64       275\n",
            "          Low Rent       0.71      0.64      0.68       533\n",
            "Medium-Priced Rent       0.72      0.78      0.75       959\n",
            "\n",
            "          accuracy                           0.71      1767\n",
            "         macro avg       0.70      0.68      0.69      1767\n",
            "      weighted avg       0.71      0.71      0.71      1767\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Define the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_neighbors': [3, 5, 7, 9],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan']\n",
        "}\n",
        "\n",
        "# Initialize the KNN classifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "\n",
        "# Perform grid search\n",
        "grid_search = GridSearchCV(knn_classifier, param_grid, cv=5, scoring='accuracy', return_train_score=True)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best hyperparameters and their corresponding scores\n",
        "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
        "print(\"Best Score:\", grid_search.best_score_)\n",
        "\n",
        "# Get the best estimator\n",
        "best_knn_classifier = grid_search.best_estimator_\n",
        "\n",
        "# Predictions on the test set\n",
        "knn_pred = best_knn_classifier.predict(X_test)\n",
        "#Save the best KNN classifier using pickle\n",
        "with open('best_knn_classifier.pkl', 'wb') as f:\n",
        "    pickle.dump(best_knn_classifier, f)\n",
        "f.close()\n",
        "# Evaluate the classifier\n",
        "print(\"KNN Classifier (Tuned):\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, knn_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, knn_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ6k2E25drGE",
        "outputId": "869722a0-8dee-495b-8f93-7f6cbf35dfd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Voting Classifier Accuracy: 0.7753254102999434\n",
            "Stacking Classifier Accuracy: 0.7804187889077533\n",
            "\n",
            " Time taken: 0 hours 5 minutes and 13.14 seconds.\n"
          ]
        }
      ],
      "source": [
        "models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
        "    ('gb', GradientBoostingClassifier(n_estimators=100)),\n",
        "    ('knn', KNeighborsClassifier(n_neighbors=5))\n",
        "]\n",
        "\n",
        "# Voting Classifier\n",
        "voting_clf = VotingClassifier(estimators=models, voting='hard')\n",
        "voting_clf.fit(X_train, y_train)\n",
        "voting_preds = voting_clf.predict(X_test)\n",
        "voting_accuracy = accuracy_score(y_test, voting_preds)\n",
        "print(\"Voting Classifier Accuracy:\", voting_accuracy)\n",
        "\n",
        "# Stacking Classifier\n",
        "stacking_clf = StackingClassifier(estimators=models, final_estimator=SVC())\n",
        "stacking_clf.fit(X_train, y_train)\n",
        "stacking_preds = stacking_clf.predict(X_test)\n",
        "stacking_accuracy = accuracy_score(y_test, stacking_preds)\n",
        "print(\"Stacking Classifier Accuracy:\", stacking_accuracy)\n",
        "\n",
        "timer(start_time) # timing ends here for \"start_time\" variable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def Classifier_test_script(data):\n",
        "  #Load the frequency map\n",
        "  with open('/content/frequency_map.pkl', 'rb') as f:\n",
        "    freq_map = pickle.load(f)\n",
        "\n",
        "\n",
        "  f1=open('/content/amenitie_function.pkl', 'rb')\n",
        "  f2=open('RentCategory_encoding.pkl', 'rb')\n",
        "  f3=open('columns_to_encode.pkl', 'rb')\n",
        "  f4=open('Bathroom_bedroom_column.pkl', 'rb')\n",
        "  f5=open('state_time_zones.pkl', 'rb')\n",
        "  f6=open('minmaxscaler.pkl', 'rb')\n",
        "  f7=open('correlation.pkl', 'rb')\n",
        "  f8=open('RentCategory_Decode.pkl', 'rb')\n",
        "  # f9=open('preprocessed_data.pkl', 'rb')\n",
        "  f10= open('body_and_title.pkl', 'rb')\n",
        "  f11= open('addressfunc.pkl', 'rb')\n",
        "  f12=open('get_mean_rooms.pkl', 'rb')\n",
        "  f13= open('get_mode_state_time.pkl', 'rb')\n",
        "  mode_cityname = open('get_mode_cityname.pkl' , 'rb')\n",
        "  mean_x_y = open('get_mean_x_y.pkl' , 'rb')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # freq_map = load(f)\n",
        "  amenitie_function = load(f1)\n",
        "  RentCategory_encoder = load(f2)\n",
        "  columns_to_encode =load(f3)\n",
        "  bathroom_bedroom_column =load(f4)\n",
        "  state_time_zones =load(f5)\n",
        "  minMaxScaler = load(f6) #####\n",
        "  corrrelationfunc = load(f7)\n",
        "  RentCategory_Decoder =load(f8)\n",
        "  # df = load(f9)\n",
        "  body_and_title=load(f10)\n",
        "  addressfunc = load(f11)\n",
        "  mean_rooms = load(f12)\n",
        "  mode_state_time =load(f13)\n",
        "  get_mode_cityname = load(mode_cityname)\n",
        "  mean_x_y = load(mean_x_y)\n",
        "\n",
        "  # implement\n",
        "  data=body_and_title(data)\n",
        "  data=amenitie_function(data)\n",
        "  data=RentCategory_encoding(data)\n",
        "  data=addressfunc(data)\n",
        "  data['bathrooms'].fillna(mean_rooms() , inplace=True)\n",
        "  data['bedrooms'].fillna(0 , inplace=True)\n",
        "  data=bathroom_bedroom_column(data)\n",
        "  data['cityname'].fillna(get_mode_cityname() , inplace=True)\n",
        "  data = freq_map(data)\n",
        "  data['state'].fillna(mode_state_time() , inplace=True)\n",
        "  mean_latitude , mean_longitude = mean_x_y()\n",
        "  data['latitude'].fillna(mean_latitude , inplace=True)\n",
        "  data['longitude'].fillna(mean_longitude , inplace=True)\n",
        "  data=columns_to_encode(data)\n",
        "  data=state_time_zones_func(data)\n",
        "  data=minMaxScaler(data)\n",
        "  data=decodefunc(data)\n",
        "\n",
        "\n",
        "  y = data['RentCategory']\n",
        "  data = data[top_feature]\n",
        "\n",
        "  return data , y"
      ],
      "metadata": {
        "id": "MzJmnv_0tJnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the new CSV file\n",
        "new_df = pd.read_csv('/content/ApartmentRentPrediction_classification_test.csv')\n",
        "X_new_data , y = Classifier_test_script(new_df)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new_data, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OUvgCvJtRxY",
        "outputId": "dcf38b91-fa3a-4c57-a58a-c2ed5ca06f9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['housing/rent/apartment' 'housing/rent/home']\n",
            "['Monthly']\n",
            "['Yes' 'Thumbnail' 'No']\n",
            "['No']\n",
            "['USD']\n",
            "['NO_PETS' 'Cats,Dogs' 'Cats' 'Dogs']\n",
            "['RentDigs.com' 'RentLingo' 'ListedBuy' 'GoSection8' 'RealRentals'\n",
            " 'Listanza' 'rentbits' 'RENTOCULAR' 'Home Rentals']\n",
            "['NC' 'NJ' 'OH' 'OK' 'CO' 'WI' 'CA' 'VA' 'TX' 'UT' 'OR' 'AZ' 'CT' 'MD'\n",
            " 'GA' 'IN' 'PA' 'MO' 'WA' 'ME' 'ND' 'FL' 'MS' 'NV' 'MA' 'KS' 'ID' 'NE'\n",
            " 'IL' 'IA' 'MI' 'MN' 'SD' 'NH' 'DC' 'LA' 'SC' 'NM' 'NY' 'AL' 'TN' 'RI'\n",
            " 'AR' 'HI' 'VT' 'AK']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_encoders.py:868: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the best models\n",
        "with open('best_dt_classifier.pkl', 'rb') as f:\n",
        "    best_dt_classifier = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "with open('best_rf_classifier.pkl', 'rb') as f:\n",
        "    best_rf_classifier = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "with open('best_lr_classifier.pkl', 'rb') as f:\n",
        "    best_lr_classifier = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "\n",
        "with open('best_knn_classifier.pkl', 'rb') as f:\n",
        "    best_knn_classifier = pickle.load(f)\n",
        "f.close()\n"
      ],
      "metadata": {
        "id": "gbT76Y0XGsiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions using the best models\n",
        "dt_pred = best_dt_classifier.predict(X_test)\n",
        "rf_pred = best_rf_classifier.predict(X_test)\n",
        "lr_pred = best_lr_classifier.predict(X_test)\n",
        "knn_pred = best_knn_classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "q6X7xbm6GpVt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the classifiers\n",
        "print(\"Decision Tree Classifier :\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, dt_pred))\n",
        "\n",
        "print(\"Random Forest Classifier :\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, rf_pred))\n",
        "\n",
        "print(\"Logistic Regression Classifier :\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, lr_pred))\n",
        "\n",
        "print(\"KNN Classifier :\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, knn_pred))"
      ],
      "metadata": {
        "id": "GD-HqnNXFXrH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03721dda-fd90-46c9-80ab-6ec972838353"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decision Tree Classifier :\n",
            "Accuracy: 0.67\n",
            "Random Forest Classifier :\n",
            "Accuracy: 0.71\n",
            "Logistic Regression Classifier :\n",
            "Accuracy: 0.615\n",
            "KNN Classifier :\n",
            "Accuracy: 0.665\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}